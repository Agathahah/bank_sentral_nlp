{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Add parent directory to path to import from src\n",
    "sys.path.append('..')\n",
    "from src.indobert_model import IndoBERTSentimentAnalyzer\n",
    "from src.utils import load_data, save_results, setup_device, plot_confusion_matrix\n",
    "from src.visualization import (plot_sentiment_time_series, plot_sentiment_distribution, \n",
    "                              create_sentiment_wordcloud, visualize_embeddings, \n",
    "                              create_interactive_dashboard)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data_path = '../data/Press_Release.xlsx'\n",
    "data = load_data(data_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Time period: {data['Tanggal'].min()} to {data['Tanggal'].max()}\")\n",
    "print(f\"Number of press releases: {data.shape[0]}\")\n",
    "\n",
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "device = setup_device()\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "# Use fine-tuned model if available\n",
    "model_path = '../models/fine-tuned-indobert/best_model'\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Using fine-tuned model from {model_path}\")\n",
    "    analyzer = IndoBERTSentimentAnalyzer(model_path=model_path)\n",
    "else:\n",
    "    print(\"Using base IndoBERT model (not fine-tuned)\")\n",
    "    analyzer = IndoBERTSentimentAnalyzer()\n",
    "\n",
    "# Load lexicon if available\n",
    "lexicon_path = '../results/lexicon/sentiment_lexicon.xlsx'\n",
    "if os.path.exists(lexicon_path):\n",
    "    print(f\"Loading sentiment lexicon from {lexicon_path}\")\n",
    "    analyzer.load_lexicon(lexicon_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze a single document\n",
    "def analyze_document(text):\n",
    "    result = analyzer.analyze_sentiment(text, use_bert=True, use_lexicon=True)\n",
    "    \n",
    "    print(f\"Sentiment: {result['label']} (Score: {result['score']:.3f})\")\n",
    "    print(f\"Analysis method: {result['method']}\")\n",
    "    \n",
    "    if 'bert_result' in result:\n",
    "        print(\"\\nBERT Analysis:\")\n",
    "        probs = result['bert_result']['probabilities']\n",
    "        for label, prob in probs.items():\n",
    "            print(f\"  {label}: {prob:.3f}\")\n",
    "    \n",
    "    if 'lexicon_result' in result:\n",
    "        print(\"\\nLexicon Analysis:\")\n",
    "        counts = result['lexicon_result']['counts']\n",
    "        print(f\"  Positive words: {counts['positif']}\")\n",
    "        print(f\"  Negative words: {counts['negatif']}\")\n",
    "        print(f\"  Neutral words: {counts['netral']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Analyze an example document\n",
    "example_idx = 0  # Change this to analyze different documents\n",
    "example_text = data.iloc[example_idx]['Isi']\n",
    "\n",
    "print(f\"Document title: {data.iloc[example_idx]['Judul']}\")\n",
    "print(f\"Date: {data.iloc[example_idx]['Tanggal']}\")\n",
    "print(\"\\nAnalysis:\")\n",
    "analysis_result = analyze_document(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Analysis of All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check if sentiment analysis has already been done\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment_Score\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming sentiment analysis on all documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Analyze all documents\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if sentiment analysis has already been done\n",
    "if 'Sentiment' not in data.columns or 'Sentiment_Score' not in data.columns:\n",
    "    print(\"Performing sentiment analysis on all documents...\")\n",
    "    \n",
    "    # Analyze all documents\n",
    "    results = analyzer.batch_analyze(data['Isi'], batch_size=4)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    data['Sentiment'] = [r['label'] for r in results]\n",
    "    data['Sentiment_Score'] = [r['score'] for r in results]\n",
    "    \n",
    "    # Save results\n",
    "    save_results(data, '../results/sentiment_results.xlsx')\n",
    "    print(\"Results saved to ../results/sentiment_results.xlsx\")\n",
    "else:\n",
    "    print(\"Using existing sentiment analysis results\")\n",
    "\n",
    "# Show distribution of sentiment categories\n",
    "sentiment_counts = data['Sentiment'].value_counts()\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Sentiment', data=data, palette='viridis')\n",
    "plt.title('Distribution of Sentiment Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze sentiment over time\n",
    "print(\"Analyzing sentiment trends over time...\")\n",
    "\n",
    "# Group by month\n",
    "data['Tanggal'] = pd.to_datetime(data['Tanggal'])\n",
    "data['Month'] = data['Tanggal'].dt.to_period('M')\n",
    "\n",
    "# Calculate average sentiment score by month\n",
    "monthly_sentiment = data.groupby('Month')['Sentiment_Score'].agg(['mean', 'count']).reset_index()\n",
    "monthly_sentiment['Month'] = monthly_sentiment['Month'].dt.to_timestamp()\n",
    "\n",
    "# Plot sentiment over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_sentiment['Month'], monthly_sentiment['mean'], 'o-', color='blue')\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.title('Sentiment Trend Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count by sentiment category per month\n",
    "sentiment_by_month = pd.crosstab(data['Month'], data['Sentiment']).reset_index()\n",
    "sentiment_by_month = sentiment_by_month.melt(id_vars=['Month'], \n",
    "                                            var_name='Sentiment', \n",
    "                                            value_name='Count')\n",
    "sentiment_by_month['Month'] = sentiment_by_month['Month'].dt.to_timestamp()\n",
    "\n",
    "# Plot stacked bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Month', y='Count', hue='Sentiment', data=sentiment_by_month, palette='viridis')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Categories Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate wordclouds by sentiment category\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Function to generate wordcloud for specific sentiment\n",
    "def generate_sentiment_wordcloud(sentiment):\n",
    "    # Filter data by sentiment\n",
    "    filtered_data = data[data['Sentiment'] == sentiment]\n",
    "    \n",
    "    if len(filtered_data) == 0:\n",
    "        print(f\"No data for sentiment: {sentiment}\")\n",
    "        return\n",
    "    \n",
    "    # Combine all text\n",
    "    text = \" \".join(filtered_data['Isi'].fillna(\"\"))\n",
    "    \n",
    "    # Create wordcloud\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=200,\n",
    "                         contour_width=1,\n",
    "                         contour_color='steelblue').generate(text)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"WordCloud for {sentiment} Sentiment\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "# Generate wordcloud for each sentiment\n",
    "for sentiment in ['Positif', 'Netral', 'Negatif']:\n",
    "    generate_sentiment_wordcloud(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-depth Analysis of Spesific Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze paragraphs in a document\n",
    "def analyze_paragraphs(document_idx):\n",
    "    if document_idx >= len(data):\n",
    "        print(f\"Error: Document index {document_idx} out of range\")\n",
    "        return\n",
    "    \n",
    "    # Get document\n",
    "    doc = data.iloc[document_idx]\n",
    "    print(f\"Analyzing document: {doc['Judul']}\")\n",
    "    print(f\"Date: {doc['Tanggal']}\")\n",
    "    print(f\"Overall sentiment: {doc['Sentiment']} (Score: {doc['Sentiment_Score']:.3f})\")\n",
    "    print(\"\\nParagraph analysis:\")\n",
    "    \n",
    "    # Analyze paragraphs\n",
    "    result = analyzer.analyze_paragraphs(doc['Isi'])\n",
    "    \n",
    "    for i, para in enumerate(result['paragraphs']):\n",
    "        print(f\"\\nParagraph {i+1}:\")\n",
    "        print(f\"Sentiment: {para['sentiment']['label']} (Score: {para['sentiment']['score']:.3f})\")\n",
    "        print(f\"Text: {para['text'][:100]}...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Analyze paragraphs in a specific document\n",
    "document_idx = 0  # Change this to analyze different documents\n",
    "paragraph_analysis = analyze_paragraphs(document_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sentiment across different time periods\n",
    "def compare_time_periods(start_date1, end_date1, start_date2, end_date2, label1=\"Period 1\", label2=\"Period 2\"):\n",
    "    # Filter data for period 1\n",
    "    period1 = data[(data['Tanggal'] >= start_date1) & (data['Tanggal'] <= end_date1)]\n",
    "    \n",
    "    # Filter data for period 2\n",
    "    period2 = data[(data['Tanggal'] >= start_date2) & (data['Tanggal'] <= end_date2)]\n",
    "    \n",
    "    print(f\"{label1}: {len(period1)} documents\")\n",
    "    print(f\"{label2}: {len(period2)} documents\")\n",
    "    \n",
    "    # Calculate sentiment statistics\n",
    "    stats1 = {\n",
    "        'mean': period1['Sentiment_Score'].mean(),\n",
    "        'median': period1['Sentiment_Score'].median(),\n",
    "        'std': period1['Sentiment_Score'].std(),\n",
    "        'positive': (period1['Sentiment'] == 'Positif').sum() / len(period1) * 100,\n",
    "        'neutral': (period1['Sentiment'] == 'Netral').sum() / len(period1) * 100,\n",
    "        'negative': (period1['Sentiment'] == 'Negatif').sum() / len(period1) * 100\n",
    "    }\n",
    "    \n",
    "    stats2 = {\n",
    "        'mean': period2['Sentiment_Score'].mean(),\n",
    "        'median': period2['Sentiment_Score'].median(),\n",
    "        'std': period2['Sentiment_Score'].std(),\n",
    "        'positive': (period2['Sentiment'] == 'Positif').sum() / len(period2) * 100,\n",
    "        'neutral': (period2['Sentiment'] == 'Netral').sum() / len(period2) * 100,\n",
    "        'negative': (period2['Sentiment'] == 'Negatif').sum() / len(period2) * 100\n",
    "    }\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\nSentiment Statistics:\")\n",
    "    print(f\"                {label1}      {label2}\")\n",
    "    print(f\"Mean Score:     {stats1['mean']:.3f}       {stats2['mean']:.3f}\")\n",
    "    print(f\"Median Score:   {stats1['median']:.3f}       {stats2['median']:.3f}\")\n",
    "    print(f\"Std Dev:        {stats1['std']:.3f}       {stats2['std']:.3f}\")\n",
    "    print(f\"% Positive:     {stats1['positive']:.1f}%      {stats2['positive']:.1f}%\")\n",
    "    print(f\"% Neutral:      {stats1['neutral']:.1f}%      {stats2['neutral']:.1f}%\")\n",
    "    print(f\"% Negative:     {stats1['negative']:.1f}%      {stats2['negative']:.1f}%\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot sentiment distribution\n",
    "    sns.histplot(period1['Sentiment_Score'], ax=ax1, label=label1, alpha=0.5, kde=True)\n",
    "    sns.histplot(period2['Sentiment_Score'], ax=ax1, label=label2, alpha=0.5, kde=True)\n",
    "    ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax1.set_xlabel('Sentiment Score')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Sentiment Score Distribution')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot sentiment category percentages\n",
    "    categories = ['Positive', 'Neutral', 'Negative']\n",
    "    period1_pcts = [stats1['positive'], stats1['neutral'], stats1['negative']]\n",
    "    period2_pcts = [stats2['positive'], stats2['neutral'], stats2['negative']]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x - width/2, period1_pcts, width, label=label1)\n",
    "    ax2.bar(x + width/2, period2_pcts, width, label=label2)\n",
    "    \n",
    "    ax2.set_xlabel('Sentiment Category')\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.set_title('Sentiment Category Distribution')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Compare sentiment before and after a specific date\n",
    "compare_time_periods(\n",
    "    pd.to_datetime('2020-01-01'), pd.to_datetime('2020-12-31'),\n",
    "    pd.to_datetime('2021-01-01'), pd.to_datetime('2021-12-31'),\n",
    "    \"2020\", \"2021\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between sentiment and other features\n",
    "# First, let's extract some additional features from the text\n",
    "data['Word_Count'] = data['Isi'].apply(lambda x: len(str(x).split()) if isinstance(x, str) else 0)\n",
    "data['Sentence_Count'] = data['Isi'].apply(lambda x: len(str(x).split('.')) if isinstance(x, str) else 0)\n",
    "data['Avg_Word_Length'] = data['Isi'].apply(lambda x: np.mean([len(word) for word in str(x).split()]) if isinstance(x, str) and len(str(x).split()) > 0 else 0)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_features = ['Sentiment_Score', 'Word_Count', 'Sentence_Count', 'Avg_Word_Length']\n",
    "corr_matrix = data[corr_features].corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Between Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(x='Word_Count', y='Sentiment_Score', data=data, hue='Sentiment', palette='viridis')\n",
    "plt.title('Sentiment vs. Word Count')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(x='Sentence_Count', y='Sentiment_Score', data=data, hue='Sentiment', palette='viridis')\n",
    "plt.title('Sentiment vs. Sentence Count')\n",
    "plt.xlabel('Sentence Count')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(x='Avg_Word_Length', y='Sentiment_Score', data=data, hue='Sentiment', palette='viridis')\n",
    "plt.title('Sentiment vs. Avg Word Length')\n",
    "plt.xlabel('Average Word Length')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive dashboard using plotly\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\"Sentiment Over Time\", \"Sentiment Distribution\", \n",
    "                       \"Sentiment by Word Count\", \"Sentiment Categories\"),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"histogram\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # Add sentiment over time\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=monthly_sentiment['Month'], \n",
    "            y=monthly_sentiment['mean'],\n",
    "            mode='lines+markers',\n",
    "            name='Average Sentiment'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add zero line\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=monthly_sentiment['Month'].min(),\n",
    "        y0=0,\n",
    "        x1=monthly_sentiment['Month'].max(),\n",
    "        y1=0,\n",
    "        line=dict(color=\"gray\", width=1, dash=\"dash\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add sentiment distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=data['Sentiment_Score'],\n",
    "            nbinsx=20,\n",
    "            name='Sentiment Distribution'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add sentiment by word count\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data['Word_Count'],\n",
    "            y=data['Sentiment_Score'],\n",
    "            mode='markers',\n",
    "            name='Sentiment by Word Count',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=data['Sentiment_Score'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add sentiment categories\n",
    "    sentiment_counts = data['Sentiment'].value_counts().reset_index()\n",
    "    sentiment_counts.columns = ['Sentiment', 'Count']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=sentiment_counts['Sentiment'],\n",
    "            y=sentiment_counts['Count'],\n",
    "            name='Sentiment Categories',\n",
    "            marker_color=['red', 'blue', 'green']\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Bank Sentral Press Release Sentiment Dashboard\",\n",
    "        height=800,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Plotly is not installed. To use the interactive dashboard, install with: pip install plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of the analysis\n",
    "def create_summary_report():\n",
    "    summary = {\n",
    "        'total_documents': len(data),\n",
    "        'time_period': f\"{data['Tanggal'].min()} to {data['Tanggal'].max()}\",\n",
    "        'sentiment_distribution': {\n",
    "            'positive': (data['Sentiment'] == 'Positif').sum(),\n",
    "            'neutral': (data['Sentiment'] == 'Netral').sum(),\n",
    "            'negative': (data['Sentiment'] == 'Negatif').sum()\n",
    "        },\n",
    "        'sentiment_stats': {\n",
    "            'mean': data['Sentiment_Score'].mean(),\n",
    "            'median': data['Sentiment_Score'].median(),\n",
    "            'std': data['Sentiment_Score'].std(),\n",
    "            'min': data['Sentiment_Score'].min(),\n",
    "            'max': data['Sentiment_Score'].max()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Find most positive and negative documents\n",
    "    most_positive_idx = data['Sentiment_Score'].idxmax()\n",
    "    most_positive = {\n",
    "        'title': data.loc[most_positive_idx, 'Judul'],\n",
    "        'date': data.loc[most_positive_idx, 'Tanggal'],\n",
    "        'score': data.loc[most_positive_idx, 'Sentiment_Score']\n",
    "    }\n",
    "    \n",
    "    most_negative_idx = data['Sentiment_Score'].idxmin()\n",
    "    most_negative = {\n",
    "        'title': data.loc[most_negative_idx, 'Judul'],\n",
    "        'date': data.loc[most_negative_idx, 'Tanggal'],\n",
    "        'score': data.loc[most_negative_idx, 'Sentiment_Score']\n",
    "    }\n",
    "    \n",
    "    summary['most_positive_document'] = most_positive\n",
    "    summary['most_negative_document'] = most_negative\n",
    "    \n",
    "    # Create report text\n",
    "    report = f\"Sentiment Analysis Report\\n{'='*50}\\n\"\n",
    "    report += f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "    \n",
    "    report += f\"Total documents analyzed: {summary['total_documents']}\\n\"\n",
    "    report += f\"Time period: {summary['time_period']}\\n\\n\"\n",
    "    \n",
    "    report += \"Sentiment Distribution:\\n\"\n",
    "    report += f\"- Positive: {summary['sentiment_distribution']['positive']} ({summary['sentiment_distribution']['positive']/summary['total_documents']*100:.1f}%)\\n\"\n",
    "    report += f\"- Neutral: {summary['sentiment_distribution']['neutral']} ({summary['sentiment_distribution']['neutral']/summary['total_documents']*100:.1f}%)\\n\"\n",
    "    report += f\"- Negative: {summary['sentiment_distribution']['negative']} ({summary['sentiment_distribution']['negative']/summary['total_documents']*100:.1f}%)\\n\\n\"\n",
    "    \n",
    "    report += \"Sentiment Score Statistics:\\n\"\n",
    "    report += f\"- Mean: {summary['sentiment_stats']['mean']:.3f}\\n\"\n",
    "    report += f\"- Median: {summary['sentiment_stats']['median']:.3f}\\n\"\n",
    "    report += f\"- Std Dev: {summary['sentiment_stats']['std']:.3f}\\n\"\n",
    "    report += f\"- Min: {summary['sentiment_stats']['min']:.3f}\\n\"\n",
    "    report += f\"- Max: {summary['sentiment_stats']['max']:.3f}\\n\\n\"\n",
    "    \n",
    "    report += \"Most Positive Document:\\n\"\n",
    "    report += f\"- Title: {summary['most_positive_document']['title']}\\n\"\n",
    "    report += f\"- Date: {summary['most_positive_document']['date']}\\n\"\n",
    "    report += f\"- Score: {summary['most_positive_document']['score']:.3f}\\n\\n\"\n",
    "    \n",
    "    report += \"Most Negative Document:\\n\"\n",
    "    report += f\"- Title: {summary['most_negative_document']['title']}\\n\"\n",
    "    report += f\"- Date: {summary['most_negative_document']['date']}\\n\"\n",
    "    report += f\"- Score: {summary['most_negative_document']['score']:.3f}\\n\\n\"\n",
    "    \n",
    "    return report, summary\n",
    "\n",
    "# Generate and display summary report\n",
    "report_text, summary_data = create_summary_report()\n",
    "print(report_text)\n",
    "\n",
    "# Save report to file\n",
    "report_path = '../results/sentiment_analysis_report.txt'\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_text)\n",
    "print(f\"Report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indobert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
