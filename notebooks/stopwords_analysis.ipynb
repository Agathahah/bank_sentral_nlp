{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Add parent directory to path to import from src\n",
    "sys.path.append('..')\n",
    "from src.utils import load_data\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = '../data/Press_Release.xlsx'\n",
    "data = load_data(data_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Time period: {data['Tanggal'].min()} to {data['Tanggal'].max()}\")\n",
    "print(f\"Number of press releases: {data.shape[0]}\")\n",
    "\n",
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Text Length and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add word count column\n",
    "data['Word_Count'] = data['Isi'].apply(lambda x: len(str(x).split()) if isinstance(x, str) else 0)\n",
    "\n",
    "# Display word count statistics\n",
    "word_count_stats = data['Word_Count'].describe()\n",
    "print(\"Word Count Statistics:\")\n",
    "print(word_count_stats)\n",
    "\n",
    "# Plot word count distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['Word_Count'], bins=30, kde=True)\n",
    "plt.axvline(x=word_count_stats['mean'], color='red', linestyle='--', label=f'Mean: {word_count_stats[\"mean\"]:.0f}')\n",
    "plt.axvline(x=word_count_stats['50%'], color='green', linestyle='--', label=f'Median: {word_count_stats[\"50%\"]:.0f}')\n",
    "plt.title('Distribution of Word Count in Press Releases')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Compare Stopwords Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard Indonesian stopwords from NLTK\n",
    "try:\n",
    "    indonesian_stopwords = set(stopwords.words('indonesian'))\n",
    "    print(f\"Standard Indonesian stopwords loaded: {len(indonesian_stopwords)} words\")\n",
    "except:\n",
    "    print(\"Indonesian stopwords not available in NLTK\")\n",
    "    indonesian_stopwords = set()\n",
    "\n",
    "# Define domain-specific stopwords for banking/finance\n",
    "banking_stopwords = {\n",
    "    'bank', 'sentral', 'bi', 'indonesia', 'gubernur', 'deputi', 'direktur',\n",
    "    'rapat', 'dewan', 'the', 'and', 'for', 'that', 'dengan', 'dalam', 'pada',\n",
    "    'dari', 'yang', 'dan', 'ini', 'itu', 'atau', 'juga', 'untuk', 'oleh', 'di',\n",
    "    'ke', 'tidak', 'akan', 'telah', 'sebagai', 'atas', 'serta', 'sedangkan',\n",
    "    'sementara', 'yaitu', 'yakni', 'bahwa', 'menteri', 'kebijakan',\n",
    "    'perekonomian', 'keuangan', 'moneter', 'fiskal', 'inflasi', 'pertumbuhan',\n",
    "    'ekonomi', 'rupiah', 'nilai', 'tukar', 'persen', 'bunga', 'suku', 'pasar'\n",
    "}\n",
    "\n",
    "# Combine stopwords\n",
    "combined_stopwords = indonesian_stopwords.union(banking_stopwords)\n",
    "print(f\"Combined stopwords: {len(combined_stopwords)} words\")\n",
    "\n",
    "# Print some examples\n",
    "print(\"\\nExamples of standard Indonesian stopwords:\")\n",
    "print(list(indonesian_stopwords)[:20])\n",
    "\n",
    "print(\"\\nExamples of banking domain stopwords:\")\n",
    "print(list(banking_stopwords)[:20])\n",
    "\n",
    "# Compare the lists\n",
    "common_words = indonesian_stopwords.intersection(banking_stopwords)\n",
    "print(f\"\\nWords appearing in both lists: {len(common_words)}\")\n",
    "if common_words:\n",
    "    print(list(common_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and Analyze Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and count words\n",
    "def tokenize_and_count_words(text, remove_stopwords=False, stopwords_list=None):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stopwords and stopwords_list:\n",
    "        tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Get all tokens from the corpus\n",
    "all_tokens_with_stopwords = []\n",
    "all_tokens_without_stopwords = []\n",
    "\n",
    "for text in data['Isi']:\n",
    "    if isinstance(text, str):\n",
    "        all_tokens_with_stopwords.extend(tokenize_and_count_words(text))\n",
    "        all_tokens_without_stopwords.extend(tokenize_and_count_words(text, True, combined_stopwords))\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq_with_stopwords = Counter(all_tokens_with_stopwords)\n",
    "word_freq_without_stopwords = Counter(all_tokens_without_stopwords)\n",
    "\n",
    "print(f\"Total tokens with stopwords: {len(all_tokens_with_stopwords)}\")\n",
    "print(f\"Total tokens without stopwords: {len(all_tokens_without_stopwords)}\")\n",
    "print(f\"Stopwords account for {(len(all_tokens_with_stopwords) - len(all_tokens_without_stopwords))/len(all_tokens_with_stopwords)*100:.1f}% of all tokens\")\n",
    "\n",
    "# Get top words\n",
    "top_words_with_stopwords = word_freq_with_stopwords.most_common(20)\n",
    "top_words_without_stopwords = word_freq_without_stopwords.most_common(20)\n",
    "\n",
    "# Create DataFrames for visualization\n",
    "top_words_with_df = pd.DataFrame(top_words_with_stopwords, columns=['Word', 'Frequency'])\n",
    "top_words_without_df = pd.DataFrame(top_words_without_stopwords, columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Top Words With and Without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top words with stopwords\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='Frequency', y='Word', data=top_words_with_df, palette='viridis')\n",
    "plt.title('Top 20 Words (Including Stopwords)')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='Frequency', y='Word', data=top_words_without_df, palette='viridis')\n",
    "plt.title('Top 20 Words (Excluding Stopwords)')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create WordCloud Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WordClouds with and without stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Function to create and display wordcloud\n",
    "def create_wordcloud(text, title, remove_stopwords=False, stopwords_list=None):\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in word_tokenize(text.lower()) if word not in stopwords_list]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=200,\n",
    "                         contour_width=1,\n",
    "                         contour_color='steelblue').generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "# Create a single text corpus from all documents\n",
    "corpus_text = ' '.join([str(text) for text in data['Isi'] if isinstance(text, str)])\n",
    "\n",
    "# Generate wordclouds\n",
    "wordcloud_with_stopwords = create_wordcloud(\n",
    "    corpus_text, \n",
    "    \"WordCloud with Stopwords\"\n",
    ")\n",
    "\n",
    "wordcloud_without_stopwords = create_wordcloud(\n",
    "    corpus_text, \n",
    "    \"WordCloud without Stopwords\",\n",
    "    remove_stopwords=True,\n",
    "    stopwords_list=combined_stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Stopwords Impact by Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the percentage of stopwords in each document\n",
    "stopword_percentages = []\n",
    "\n",
    "for text in data['Isi']:\n",
    "    if isinstance(text, str):\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Count stopwords\n",
    "        stopword_count = sum(1 for token in tokens if token in combined_stopwords)\n",
    "        \n",
    "        # Calculate percentage\n",
    "        if len(tokens) > 0:\n",
    "            percentage = stopword_count / len(tokens) * 100\n",
    "        else:\n",
    "            percentage = 0\n",
    "        \n",
    "        stopword_percentages.append(percentage)\n",
    "    else:\n",
    "        stopword_percentages.append(0)\n",
    "\n",
    "# Add to dataframe\n",
    "data['Stopword_Percentage'] = stopword_percentages\n",
    "\n",
    "# Display statistics\n",
    "stopword_stats = data['Stopword_Percentage'].describe()\n",
    "print(\"Stopword Percentage Statistics:\")\n",
    "print(stopword_stats)\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['Stopword_Percentage'], bins=20, kde=True)\n",
    "plt.axvline(x=stopword_stats['mean'], color='red', linestyle='--', label=f'Mean: {stopword_stats[\"mean\"]:.1f}%')\n",
    "plt.axvline(x=stopword_stats['50%'], color='green', linestyle='--', label=f'Median: {stopword_stats[\"50%\"]:.1f}%')\n",
    "plt.title('Distribution of Stopword Percentage in Press Releases')\n",
    "plt.xlabel('Stopword Percentage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Over Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stopword usage over time\n",
    "data['Year'] = pd.to_datetime(data['Tanggal']).dt.year\n",
    "data['Month'] = pd.to_datetime(data['Tanggal']).dt.month\n",
    "\n",
    "# Group by year\n",
    "stopwords_by_year = data.groupby('Year')['Stopword_Percentage'].mean().reset_index()\n",
    "\n",
    "# Plot stopword percentage by year\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Year', y='Stopword_Percentage', data=stopwords_by_year, marker='o')\n",
    "plt.title('Average Stopword Percentage by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Stopword Percentage')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if stopword usage is correlated with document length\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Word_Count', y='Stopword_Percentage', data=data)\n",
    "plt.title('Stopword Percentage vs. Document Length')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Stopword Percentage')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Stopwords Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which stopwords are most common in the corpus\n",
    "stopword_counts = {}\n",
    "\n",
    "for stopword in combined_stopwords:\n",
    "    count = sum(1 for token in all_tokens_with_stopwords if token == stopword)\n",
    "    stopword_counts[stopword] = count\n",
    "\n",
    "# Convert to DataFrame\n",
    "stopword_freq = pd.DataFrame(list(stopword_counts.items()), columns=['Stopword', 'Frequency'])\n",
    "stopword_freq = stopword_freq.sort_values('Frequency', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top stopwords\n",
    "print(\"Top 20 most frequent stopwords in the corpus:\")\n",
    "print(stopword_freq.head(20))\n",
    "\n",
    "# Plot top stopwords\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Frequency', y='Stopword', data=stopword_freq.head(20), palette='viridis')\n",
    "plt.title('Top 20 Most Frequent Stopwords')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Stopwords List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate stopwords impact\n",
    "def evaluate_stopword_impact(text, stopword):\n",
    "    \"\"\"Calculate impact of removing a single stopword\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    count = sum(1 for token in tokens if token == stopword)\n",
    "    return count / len(tokens) * 100 if len(tokens) > 0 else 0\n",
    "\n",
    "# Evaluate impact of top stopwords\n",
    "top_stopwords = stopword_freq.head(30)['Stopword'].tolist()\n",
    "impact_data = []\n",
    "\n",
    "for stopword in top_stopwords:\n",
    "    # Calculate average impact across documents\n",
    "    avg_impact = data['Isi'].apply(lambda x: evaluate_stopword_impact(x, stopword)).mean()\n",
    "    impact_data.append({'Stopword': stopword, 'Impact': avg_impact})\n",
    "\n",
    "impact_df = pd.DataFrame(impact_data)\n",
    "impact_df = impact_df.sort_values('Impact', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Impact of removing top stopwords:\")\n",
    "print(impact_df.head(20))\n",
    "\n",
    "# Plot impact\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Impact', y='Stopword', data=impact_df.head(20), palette='viridis')\n",
    "plt.title('Impact of Removing Top Stopwords (% of Text Removed)')\n",
    "plt.xlabel('Average Impact (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Stopwords List for Banking Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimized stopwords list\n",
    "# Keep only stopwords with significant impact\n",
    "impact_threshold = 0.1  # Minimum impact percentage to include in optimized list\n",
    "optimized_stopwords = impact_df[impact_df['Impact'] >= impact_threshold]['Stopword'].tolist()\n",
    "\n",
    "# Add domain-specific stopwords that might not be frequent but are irrelevant for analysis\n",
    "domain_specific = [\n",
    "    'bank', 'sentral', 'bi', 'indonesia', 'gubernur', 'deputi', 'direktur',\n",
    "    'rapat', 'dewan', 'kebijakan', 'moneter', 'keuangan',\n",
    "    'pertemuan', 'triwulan', 'kuartal', 'persen', 'tanggal', 'bulan'\n",
    "]\n",
    "\n",
    "# Combine lists\n",
    "final_stopwords = set(optimized_stopwords + domain_specific)\n",
    "print(f\"Final optimized stopwords list: {len(final_stopwords)} words\")\n",
    "print(sorted(final_stopwords))\n",
    "\n",
    "# Save the optimized stopwords list\n",
    "stopwords_path = '../results/optimized_stopwords.txt'\n",
    "os.makedirs(os.path.dirname(stopwords_path), exist_ok=True)\n",
    "\n",
    "with open(stopwords_path, 'w') as f:\n",
    "    for word in sorted(final_stopwords):\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "print(f\"Optimized stopwords list saved to {stopwords_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Text Processing With Different Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results of different stopwords lists\n",
    "example_text = data['Isi'].iloc[0] if len(data) > 0 else \"Contoh teks tidak tersedia\"\n",
    "\n",
    "# Define different stopwords lists to compare\n",
    "stopwords_lists = {\n",
    "    'No Stopwords': set(),\n",
    "    'NLTK Indonesian': indonesian_stopwords,\n",
    "    'Domain-specific': banking_stopwords,\n",
    "    'Combined': combined_stopwords,\n",
    "    'Optimized': final_stopwords\n",
    "}\n",
    "\n",
    "print(\"Example text:\")\n",
    "print(example_text[:500] + \"...\\n\")\n",
    "\n",
    "print(\"Text after removing different stopwords lists:\")\n",
    "for name, sw_list in stopwords_lists.items():\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(example_text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in sw_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # Calculate reduction\n",
    "    reduction = (1 - len(filtered_tokens) / len(tokens)) * 100\n",
    "    \n",
    "    print(f\"\\n{name} ({len(sw_list)} words, {reduction:.1f}% reduction):\")\n",
    "    print(filtered_text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate impact of different stopwords lists on corpus\n",
    "evaluation_results = []\n",
    "\n",
    "for name, sw_list in stopwords_lists.items():\n",
    "    # Count tokens with and without stopwords\n",
    "    total_tokens = 0\n",
    "    remaining_tokens = 0\n",
    "    \n",
    "    for text in data['Isi']:\n",
    "        if isinstance(text, str):\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            total_tokens += len(tokens)\n",
    "            \n",
    "            filtered_tokens = [word for word in tokens if word not in sw_list]\n",
    "            remaining_tokens += len(filtered_tokens)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    reduction = (1 - remaining_tokens / total_tokens) * 100\n",
    "    evaluation_results.append({\n",
    "        'Stopwords List': name,\n",
    "        'List Size': len(sw_list),\n",
    "        'Total Tokens': total_tokens,\n",
    "        'Remaining Tokens': remaining_tokens,\n",
    "        'Reduction (%)': reduction\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "print(\"Evaluation of different stopwords lists:\")\n",
    "print(eval_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Stopwords List', y='Reduction (%)', data=eval_df, palette='viridis')\n",
    "plt.title('Text Reduction by Different Stopwords Lists')\n",
    "plt.xlabel('Stopwords List')\n",
    "plt.ylabel('Reduction (%)')\n",
    "plt.ylim(0, 100)\n",
    "for i, v in enumerate(eval_df['Reduction (%)']):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final recommendations\n",
    "print(\"Recommendations for Stopwords in Bank Sentral NLP Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Most Effective Stopwords List\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Identify most effective list based on size vs. reduction\n",
    "efficiency = eval_df['Reduction (%)'] / eval_df['List Size']\n",
    "most_efficient_idx = efficiency.idxmax()\n",
    "most_efficient = eval_df.iloc[most_efficient_idx]['Stopwords List']\n",
    "\n",
    "print(f\"Most efficient stopwords list: {most_efficient}\")\n",
    "print(f\"Provides {eval_df.iloc[most_efficient_idx]['Reduction (%)]:.1f}% text reduction with only {eval_df.iloc[most_efficient_idx]['List Size']} words\")\n",
    "\n",
    "print(\"\\n2. Key Stopwords to Always Include\")\n",
    "print(\"-\" * 40)\n",
    "print(\"The following stopwords have the highest impact and should always be included:\")\n",
    "for _, row in impact_df.head(10).iterrows():\n",
    "    print(f\"- {row['Stopword']}: {row['Impact']:.2f}% impact\")\n",
    "\n",
    "print(\"\\n3. Domain-Specific Consideration\")\n",
    "print(\"-\" * 40)\n",
    "print(\"For Bank Sentral communication analysis, consider the context:\")\n",
    "print(\"- Some common banking terms might be stopwords in general analysis but meaningful for specific tasks\")\n",
    "print(\"- Terms like 'inflasi', 'suku bunga' might be stopwords for document classification but crucial for sentiment analysis\")\n",
    "\n",
    "print(\"\\n4. Final Recommendation\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Use the optimized stopwords list for general preprocessing, but maintain flexibility:\")\n",
    "print(f\"- {len(final_stopwords)} optimized stopwords provide good balance between reduction and specificity\")\n",
    "print(\"- For topic modeling: use more aggressive stopwords removal\")\n",
    "print(\"- For sentiment analysis: use more conservative stopwords removal\")\n",
    "print(\"- Consider task-specific stopwords lists for different analysis goals\")\n",
    "\n",
    "# Save recommendations\n",
    "recommendations_path = '../results/stopwords_recommendations.txt'\n",
    "with open(recommendations_path, 'w') as f:\n",
    "    f.write(\"Recommendations for Stopwords in Bank Sentral NLP Analysis\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. Most Effective Stopwords List\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Most efficient stopwords list: {most_efficient}\\n\")\n",
    "    f.write(f\"Provides {eval_df.iloc[most_efficient_idx]['Reduction (%)']:.1f}% text reduction with only {eval_df.iloc[most_efficient_idx]['List Size']} words\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. Key Stopwords to Always Include\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(\"The following stopwords have the highest impact and should always be included:\\n\")\n",
    "    for _, row in impact_df.head(10).iterrows():\n",
    "        f.write(f\"- {row['Stopword']}: {row['Impact']:.2f}% impact\\n\")\n",
    "    \n",
    "    f.write(\"\\n3. Domain-Specific Consideration\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(\"For Bank Sentral communication analysis, consider the context:\\n\")\n",
    "    f.write(\"- Some common banking terms might be stopwords in general analysis but meaningful for specific tasks\\n\")\n",
    "    f.write(\"- Terms like 'inflasi', 'suku bunga' might be stopwords for document classification but crucial for sentiment analysis\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. Final Recommendation\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(\"Use the optimized stopwords list for general preprocessing, but maintain flexibility:\\n\")\n",
    "    f.write(f\"- {len(final_stopwords)} optimized stopwords provide good balance between reduction and specificity\\n\")\n",
    "    f.write(\"- For topic modeling: use more aggressive stopwords removal\\n\")\n",
    "    f.write(\"- For sentiment analysis: use more conservative stopwords removal\\n\")\n",
    "    f.write(\"- Consider task-specific stopwords lists for different analysis goals\\n\")\n",
    "\n",
    "print(f\"\\nRecommendations saved to {recommendations_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indobert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
